{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer, MaxAbsScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "data = pd.read_csv(DATA_PATH+\"2016_Building_Energy_Benchmarking.csv\", sep=\",\", encoding=\"iso-8859-1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_info_rows',50)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().mean()[data.isna().mean() > 0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1 = \"TotalGHGEmissions\"\n",
    "label_2 = \"SiteEnergyUse(kBtu)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = list(data.columns)\n",
    "categorical_columns = [\"BuildingType\", \"PrimaryPropertyType\",\"PropertyName\",\"City\",\"State\",\"Address\",\"City\",\"State\",\n",
    "                       \"ZipCode\",\"CouncilDistrictCode\",\"Neighborhood\",\"ListOfAllPropertyUseTypes\",\n",
    "                       \"LargestPropertyUseType\",\"SecondLargestPropertyUseType\",\"ThirdLargestPropertyUseType\",\"ComplianceStatus\"]\n",
    "numerical_columns = [column for column in all_columns if column not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premier nettoyage du dataframe\n",
    "def clean_data(df, to_drop):\n",
    "    print(\"--------- shape before cleaning ---------\")\n",
    "    print(df.shape)\n",
    "    df = df[df.ComplianceStatus == \"Compliant\"]\n",
    "    df.Outlier.fillna('not_outlier', inplace=True)\n",
    "    df.SecondLargestPropertyUseType.fillna('one_use', inplace=True)\n",
    "    df.ThirdLargestPropertyUseType.fillna('one_use', inplace=True)\n",
    "    df = df[df.NumberofBuildings != 0]\n",
    "    df = df[df.NumberofFloors != 0]\n",
    "    df = df[df['DefaultData'] == False]\n",
    "    df.loc[df['SecondLargestPropertyUseType'] == 'one_use', 'SecondLargestPropertyUseTypeGFA'] = 0\n",
    "    df.loc[df['ThirdLargestPropertyUseType'] == 'one_use', 'ThirdLargestPropertyUseTypeGFA'] = 0\n",
    "    df['Nb_PropertyUseTypes'] = df['ListOfAllPropertyUseTypes'].str.count(',') + 1\n",
    "    df[\"Building_age\"] = df[\"DataYear\"] - df[\"YearBuilt\"]\n",
    "    df['LargestPropertyUseType'] = df['LargestPropertyUseType'].fillna(df['PrimaryPropertyType'])\n",
    "    df['LargestPropertyUseTypeGFA'] = df['LargestPropertyUseTypeGFA'].fillna(df['PropertyGFATotal'])\n",
    "    df[\"%_LargestPropertyUseType\"] = df[\"LargestPropertyUseTypeGFA\"] / df[\"PropertyGFATotal\"]    \n",
    "    for col in to_drop:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "    print(\"--------- shape before cleaning ---------\")\n",
    "    print(df.shape)\n",
    "    return df\n",
    "\n",
    "# Mise à jour des listes des colonnes\n",
    "def update_columns_list(list_1, dropped):\n",
    "    new_list = [col for col in list_1 if col not in dropped]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier la GFA\n",
    "data[\"GFATotal\"] = data[\"PropertyGFAParking\"] + data[\"PropertyGFABuilding(s)\"]\n",
    "check_GPA = pd.Series(data[\"GFATotal\"] == data[\"PropertyGFATotal\"])\n",
    "data.drop(columns=[\"GFATotal\"], inplace=True)\n",
    "check_GPA.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data.BuildingType.isin([\"Multifamily LR (1-4)\",\"Multifamily MR (5-9)\",\"Multifamily HR (10+)\"])]\n",
    "data[\"%_GFAParking\"] = data[\"PropertyGFAParking\"] / data[\"PropertyGFATotal\"]\n",
    "data[\"%_GFABuilding\"] = data[\"PropertyGFABuilding(s)\"] / data[\"PropertyGFATotal\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"DataYear\",\"PropertyName\",\"Address\",\"City\",\"State\",\n",
    "                \"Latitude\",\"Longitude\",\"YearBuilt\",\"TaxParcelIdentificationNumber\",\n",
    "                \"YearsENERGYSTARCertified\",\"Comments\",\"DefaultData\",\"ComplianceStatus\",\n",
    "                \"SiteEnergyUseWN(kBtu)\",\"PropertyGFAParking\",\"PropertyGFABuilding(s)\",\"ListOfAllPropertyUseTypes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_data(data, columns_to_drop)\n",
    "categorical_columns = update_columns_list(categorical_columns,columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.BuildingType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',50)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ENERGYSTARScore = data.ENERGYSTARScore.fillna('no_score')\n",
    "data = data[data[\"ENERGYSTARScore\"] != 'no_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['BuildingType', 'PrimaryPropertyType', 'ZipCode',\n",
    "       'Neighborhood',  'Building_age','NumberofBuildings',\n",
    "       'NumberofFloors', 'PropertyGFATotal', 'LargestPropertyUseType',\n",
    "       'LargestPropertyUseTypeGFA',  '%_LargestPropertyUseType',\n",
    "       'SourceEUI(kBtu/sf)',\n",
    "       'SiteEnergyUse(kBtu)', 'SteamUse(kBtu)','Electricity(kBtu)','NaturalGas(kBtu)',\n",
    "       'TotalGHGEmissions',\n",
    "       '%_GFAParking', '%_GFABuilding','Nb_PropertyUseTypes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = update_columns_list(categorical_columns,columns_to_drop)\n",
    "numerical_columns = update_columns_list(numerical_columns,columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[features]\n",
    "cat_features = [col for col in features if col in categorical_columns]\n",
    "num_features = [col for col in features if col in categorical_columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"TotalGHGEmissions\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TotalGHGEmissions\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TotalGHGEmissions\"].quantile([0, .5, 0.98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"TotalGHGEmissions\"] <= df[\"TotalGHGEmissions\"].quantile([0.98])[0.98]]\n",
    "df = df[df[\"TotalGHGEmissions\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[label_2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label_1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label_2].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BuildingType.nunique(), df.PrimaryPropertyType.nunique(), df.ZipCode.nunique(), df.Neighborhood.nunique(), df.LargestPropertyUseType.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_ohe(df):\n",
    "    \n",
    "    #### encoder la colonne LargestPropertyUseType ###\n",
    "    # Ajouter les colonnes encodées au dataframe d'origine\n",
    "    # Supprimer la colonne originale\n",
    "    onehot_encoded = pd.get_dummies(df['LargestPropertyUseType'], prefix='LargestPropertyUseType')\n",
    "    df = pd.concat([df, onehot_encoded], axis=1)\n",
    "    df.drop('LargestPropertyUseType', axis=1, inplace=True)\n",
    "    \n",
    "    #### encoder la colonne BuildingType ###\n",
    "    # Ajouter les colonnes encodées au dataframe d'origine\n",
    "    # Supprimer la colonne originale\n",
    "    onehot_encoded_BuildingType = pd.get_dummies(df['BuildingType'], prefix='BuildingType')\n",
    "    df = pd.concat([df, onehot_encoded_BuildingType], axis=1)\n",
    "    df.drop('BuildingType', axis=1, inplace=True)\n",
    "    \n",
    "    ### Remplacer les valeurs 1/0 de LargestPropertyUseType_ par leur % de la surface totale\n",
    "    # Liste des colonnes à remplacer\n",
    "    # Boucle pour remplacer les valeurs dans chaque colonne\n",
    "    cols_to_replace = list(df.filter(like=\"LargestPropertyUseType_\").columns)\n",
    "    for col in cols_to_replace:\n",
    "        df[col] = df.apply(lambda row: row[\"%_LargestPropertyUseType\"] if row[col] == 1 else 0, axis=1)    \n",
    "    df.drop(columns=[\"%_LargestPropertyUseType\"], inplace=True)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pipe_ohe(df)\n",
    "y1 = df[[label_1]]\n",
    "y2 = df[[label_2]]\n",
    "\n",
    "X_enrg_train, X_enrg_test, y_nrg_train, y_nrg_test = train_test_split(X,y1,test_size=0.15, random_state=42, stratify=df[\"BuildingType\"])\n",
    "print(X_enrg_train.shape, X_enrg_test.shape)\n",
    "\n",
    "# séparer la donnée pour avoir les \n",
    "X_ghge_train, X_ghge_test, y_ghe_train, y_ghe_test = train_test_split(X,y2,test_size=0.15, random_state=42, stratify=df[\"BuildingType\"])\n",
    "print(X_ghge_train.shape, X_ghge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_X_enrg_test = X_enrg_test.index\n",
    "df_nrg_selection = df.loc[index_X_enrg_test]\n",
    "\n",
    "index_X_ghge_test = X_ghge_test.index\n",
    "df_ghe_selection = df.loc[index_X_ghge_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(df, means_zipcode=None, \n",
    "                means_neighborhood=None,\n",
    "                means_ppropertype=None, \n",
    "                col_for_mean=\"PropertyGFATotal\", \n",
    "                scaler=None,\n",
    "                label_1=label_1,\n",
    "                label_2=label_2):\n",
    "    #### encoder la colonne ZipCode ###\n",
    "    # encoding ZipCode with mean encoding and TotalGHGEmissions #\n",
    "    # Calculer la moyenne de la colonne cible (label2) pour chaque code postal\n",
    "    # Encoder la colonne code postal en fonction de la moyenne de la colonne cible\n",
    "    # Supprimer la colonne originale\n",
    "    mean_encoding_nan = df[col_for_mean].mean()\n",
    "    if means_zipcode is None:\n",
    "        means_zipcode = df.groupby('ZipCode')[col_for_mean].mean()\n",
    "    df['Code_postal_encoded'] = df['ZipCode'].map(means_zipcode)\n",
    "    df['Code_postal_encoded'].fillna(mean_encoding_nan, inplace=True)\n",
    "    \n",
    "    ### encoder la colonne Neighborhood ###\n",
    "    # Calculer la moyenne de la colonne cible (label2) pour chaque quartier\n",
    "    # Encoder la colonne quartier en fonction de la moyenne de la colonne cible\n",
    "    # Supprimer la colonne originale\n",
    "    if means_neighborhood is None:\n",
    "        means_neighborhood = df.groupby('Neighborhood')[col_for_mean].mean()\n",
    "    df['neighborhood_encoded'] = df['Neighborhood'].map(means_neighborhood)\n",
    "    df.drop(columns=['Neighborhood'],inplace=True)\n",
    "    df['neighborhood_encoded'].fillna(mean_encoding_nan, inplace=True)\n",
    "    \n",
    "    \n",
    "    ### encoder la colonne PrimaryPropertyType with mean encoding  ###\n",
    "    # Calculer la moyenne de la colonne cible (label2) pour chaque PrimaryPropertyType\n",
    "    # Encoder la colonne PrimaryPropertyType en fonction de la moyenne de la colonne cible\n",
    "    # Supprimer la colonne catégorielle\n",
    "    if means_ppropertype is None:\n",
    "        means_ppropertype = df.groupby('PrimaryPropertyType')[col_for_mean].mean()\n",
    "    df['PrimaryPropertyType_encoded'] = df['PrimaryPropertyType'].map(means_ppropertype)\n",
    "    df.drop(columns=['PrimaryPropertyType'],inplace=True)\n",
    "    df['PrimaryPropertyType_encoded'].fillna(mean_encoding_nan, inplace=True)\n",
    "    \n",
    "    #supprimer les labels du jeu de données\n",
    "    df = df.drop(columns=[label_1,label_2])\n",
    "    df.drop(columns=['SourceEUI(kBtu/sf)', 'SteamUse(kBtu)','Electricity(kBtu)', 'NaturalGas(kBtu)'], inplace=True)\n",
    "    \n",
    "    #scaler\n",
    "    # if scaler is None:\n",
    "    #     scaler=StandardScaler()\n",
    "    #     df_scaled = scaler.fit_transform(df)\n",
    "    #     print(\"check scaler standard scaler\")\n",
    "    # else:\n",
    "    #     df_scaled = scaler.transform(df)\n",
    "    #     print(\"check scaler the train scaler\")\n",
    "    \n",
    "    return df, means_zipcode, means_neighborhood, means_ppropertype, df.columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nrg_train, means_zipcode_enrg, means_neighborhood_enrg, means_ppropertype_enrg, columns_enrg = pipe(X_enrg_train,col_for_mean=label_1)\n",
    "X_nrg_test, means_zipcode_enrg, means_neighborhood_enrg, means_ppropertype_enrg, a = pipe(X_enrg_test,\n",
    "                                                                        means_zipcode=means_zipcode_enrg,\n",
    "                                                                        means_neighborhood=means_neighborhood_enrg,\n",
    "                                                                        means_ppropertype=means_ppropertype_enrg,\n",
    "                                                                        col_for_mean=label_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ghe_train, means_zipcode_ghe, means_neighborhood_ghe, means_ppropertype_ghe, columns_ghe = pipe(X_ghge_train,col_for_mean=label_2)\n",
    "X_ghe_test, means_zipcode_ghe, means_neighborhood_ghe, means_ppropertype_ghe, b = pipe(X_ghge_test, \n",
    "                                                                        means_zipcode=means_zipcode_ghe,\n",
    "                                                                        means_neighborhood=means_neighborhood_ghe,\n",
    "                                                                        means_ppropertype=means_ppropertype_ghe,\n",
    "                                                                        col_for_mean=label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_nrg_train.shape, X_nrg_test.shape), \n",
    "print(X_ghe_train.shape, X_ghe_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Results functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_printed(model):\n",
    "    # Best score                     \n",
    "    print(f\"Best score : {model.best_score_}\")\n",
    "    \n",
    "    # Best Params\n",
    "    print(f\"Best params :\\n {model.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_nrg = pd.DataFrame(columns=[\"label\",\"modèle\",\"r2_train\",\"r2_test\",\"mae\",\"rmse\",\"mean_fit_time\",\"best_params\"])\n",
    "metrics_df_ghe = pd.DataFrame(columns=[\"label\",\"modèle\",\"r2_train\",\"r2_test\",\"mae\",\"rmse\",\"mean_fit_time\",\"best_params\"])\n",
    "\n",
    "def add_metrics_table(label,y_pred,y_test,df,model, X_train, y_train):\n",
    "    r2_train = round(model.score(X_train, y_train,),4)\n",
    "    r2_test = round(r2_score(y_test, y_pred), 4)\n",
    "    mae = round(mean_absolute_error(y_test, y_pred), 4)\n",
    "    rmse = round(mean_squared_error(y_test, y_pred), 4)\n",
    "    df = df.append({\n",
    "    \"label\": label,\n",
    "    \"modèle\": model.best_estimator_,\n",
    "    \"r2_train\":'{:.3f}'.format(r2_train),\n",
    "    \"r2_test\": '{:.3f}'.format(r2_test),\n",
    "    \"mae\": '{:.3f}'.format(mae),\n",
    "    \"rmse\": '{:.3f}'.format(rmse),\n",
    "    \"mean_fit_time\": model.cv_results_['mean_fit_time'].mean(),\n",
    "    \"best_params\": model.best_params_\n",
    "    }, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "scoring = [\"r2\", \"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"]\n",
    "\n",
    "def evaluate_prediction(label, X_test, y_test, metrics_df, model, X_train, y_train):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics_df = add_metrics_table(label, y_pred, y_test, metrics_df, model, X_train, y_train)\n",
    "\n",
    "    # Créer le scatter plot\n",
    "    plt.scatter(y_pred, y_test)\n",
    "\n",
    "    # Ajouter une ligne diagonale pour représenter la ligne de prédiction parfaite\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "\n",
    "    # Ajouter des labels pour les axes et le titre du plot\n",
    "    plt.xlabel('Prédictions')\n",
    "    plt.ylabel('Valeurs réelles')\n",
    "    plt.title('Comparaison des prédictions et des valeurs réelles')\n",
    "\n",
    "    # Afficher le plot\n",
    "    plt.show()\n",
    "    display(metrics_df)\n",
    "    return metrics_df, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_errors(y_test, y_pred):\n",
    "    y_test = np.array(y_test).reshape(-1,)\n",
    "    diff = y_test - y_pred\n",
    "    mean = diff.mean()\n",
    "    std = diff.std()\n",
    "    plt.hist(diff, bins=40)\n",
    "    plt.vlines(mean, 0, 30, color='red', label=f'mean = {mean:.2f}')\n",
    "    plt.hlines(30, mean - 1/2 * std, mean + 1/2 * std, color='red', label=f'std = {std:.2f}', ls='dotted')\n",
    "    plt.title('Histogram of prediction errors')\n",
    "    plt.xlabel('prediction error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Showing the legend\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fetures_coef(model, columns):\n",
    "    coef = model.best_estimator_.coef_\n",
    "    if coef.ndim == 2 and coef.shape[0] == 1:\n",
    "        coef = coef[0]\n",
    "    features_coef = pd.DataFrame(coef.reshape(1,-1), columns=list(columns))\n",
    "    columns = list(reversed(features_coef.columns))\n",
    "    coef = list(reversed(coef))\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(14,8))\n",
    "    # Créer un graphique à barres horizontales\n",
    "    plt.barh(columns, coef)\n",
    "\n",
    "    # Récupérer les étiquettes de l'axe des abscisses\n",
    "    labels = plt.gca().get_yticklabels()\n",
    "\n",
    "    # Parcourir les étiquettes et appliquer un style différent aux étiquettes non nulles\n",
    "    for label, value in zip(labels, coef):\n",
    "        if value != -0:\n",
    "            label.set_color('red')  # Appliquer une couleur rouge aux étiquettes non nulles\n",
    "        else:\n",
    "            label.set_color('black')  # Appliquer une couleur noire aux étiquettes nulles\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_per_category(df, y_test, y_pred, feature):\n",
    "    df[\"error_pred\"] = (np.array(y_test).reshape(-1,) - y_pred)\n",
    "    df_grouped = df.groupby(feature).mean()['error_pred']\n",
    "    counts = df[feature].value_counts()\n",
    "\n",
    "    # Créer le graphique montrant la moyenne des erreurs et le nombre d'occurrences\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Afficher les barres de la moyenne des erreurs\n",
    "    ax.bar(df_grouped.index, df_grouped.values)\n",
    "\n",
    "    # Ajouter le nombre d'occurrences sur les barres de la moyenne des erreurs\n",
    "    for i, v in enumerate(df_grouped.values):\n",
    "        ax.text(i, v, str(counts[df_grouped.index[i]]), ha='center', va='bottom')\n",
    "\n",
    "    # Ajouter une légende\n",
    "    ax.text(0.05, -0.2, f'{counts[df_grouped.index[i]]}:Nombre d\\'occurrences dans le jeu de test', transform=ax.transAxes)\n",
    "\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Moyenne des erreurs')\n",
    "\n",
    "    # Changer la taille de la figure\n",
    "    fig.set_size_inches(8, 4)\n",
    "    \n",
    "    # Ajouter un espace supplémentaire de 10% au-dessus et en dessous des barres du graphique\n",
    "    ax.margins(y=0.1)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_all_category(df, y_test, y_pred, features):\n",
    "    # Créer une figure avec des subplots\n",
    "    fig, axs = plt.subplots(nrows=len(features), ncols=1, figsize=(8, 4 * len(features)), gridspec_kw={'hspace': 0.6})\n",
    "    axs[0].set_ylabel('Moyenne des erreurs')\n",
    "\n",
    "    # Boucle sur les features\n",
    "    for i, feature in enumerate(features):\n",
    "        # Récupérer l'axe courant\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Calculer les erreurs moyennes pour chaque catégorie\n",
    "        df[\"error_pred\"] = abs((np.array(y_test).reshape(-1,) - y_pred))\n",
    "        df_grouped = df.groupby(feature).mean()['error_pred']\n",
    "        counts = df_nrg_selection[feature].value_counts()\n",
    "\n",
    "        # Afficher les barres de la moyenne des erreurs\n",
    "        ax.bar(df_grouped.index, df_grouped.values)\n",
    "\n",
    "        # Ajouter le nombre d'occurrences sur les barres de la moyenne des erreurs\n",
    "        for i, v in enumerate(df_grouped.values):\n",
    "            ax.text(i, v, str(counts[df_grouped.index[i]]), ha='center', va='bottom')\n",
    "\n",
    "        # Ajouter un titre pour chaque subplot\n",
    "        ax.set_title(feature)\n",
    "\n",
    "        # Ajouter un espace supplémentaire de 10% au-dessus et en dessous des barres du graphique\n",
    "        ax.margins(y=0.1)\n",
    "        \n",
    "        # Ajouter des étiquettes d'axe plus petites et penchées\n",
    "        ax.set_xticklabels(df_grouped.index, rotation=45, fontsize=7)\n",
    "\n",
    "    # Ajouter un titre pour le graphique global\n",
    "    fig.suptitle('Erreurs en fonction des variables catégorielles', y=0.95, fontsize=16)\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_discrete_feature(df, col):\n",
    "    # Créer un graphique en nuage de points pour représenter les erreurs\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(df[col], df['error_pred'], label=col)\n",
    "\n",
    "    # Ajouter les labels des axes et la légende\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('error_pred')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_all_discr_category(df, y_test, y_pred, features):\n",
    "    # Créer une figure avec des subplots\n",
    "    fig, axs = plt.subplots(nrows=len(features), ncols=1, figsize=(8, 4 * len(features)), gridspec_kw={'hspace': 0.3})\n",
    "\n",
    "    # Calculer les erreurs moyennes pour chaque catégorie\n",
    "    df[\"error_pred\"] = abs((np.array(y_test).reshape(-1,) - y_pred))\n",
    "        \n",
    "    # Boucle sur les features\n",
    "    for i, feature in enumerate(features):\n",
    "        # Récupérer l'axe courant\n",
    "        ax = axs[i]\n",
    "\n",
    "        ax.scatter(df[feature], df['error_pred'], label=feature)\n",
    "\n",
    "        # Ajouter un titre pour chaque subplot\n",
    "        ax.set_title(feature)\n",
    "\n",
    "    # Ajouter un titre pour le graphique global\n",
    "    fig.suptitle('Erreurs en fonction des variables catégorielles', y=0.92, fontsize=16)\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n",
    "\n",
    "disc_col_viz = [\"Building_age\",\"%_GFAParking\",\"%_GFABuilding\",\"LargestPropertyUseTypeGFA\",\"NumberofBuildings\",\"NumberofFloors\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline with a scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label 1 : energy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor()\n",
    "parameters = {\"strategy\" : (\"mean\",\"median\",\"quantile\"),\n",
    "            \"quantile\" : [0.25,0.5,0.75]}\n",
    "grid_dummy_nrg = GridSearchCV(dummy_regr, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit = \"r2\")\n",
    "grid_dummy_nrg.fit(X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_nrg, y_pred_dummy = evaluate_prediction(label_1,X_nrg_test, y_nrg_test, metrics_df_nrg, grid_dummy_nrg,X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "parameters = {'fit_intercept': [True, False]}\n",
    "grid_reglin_nrg = GridSearchCV(reg,\n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_reglin_nrg.fit(X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_nrg, y_pred_reg = evaluate_prediction(label_1, X_nrg_test, y_nrg_test, metrics_df_nrg, grid_reglin_nrg,X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(random_state=42)\n",
    "alpha_space = np.logspace(-4, 2, 15)   # Checking for alpha from .0001 to 1 and finding the best value for alpha\n",
    "parameters = {\"alpha\" : alpha_space}\n",
    "grid_lasso_nrg = GridSearchCV(lasso, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_lasso_nrg.fit(X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_nrg, y_pred_lasso = evaluate_prediction(label_1,X_nrg_test, y_nrg_test, metrics_df_nrg, grid_lasso_nrg, X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fetures_coef(grid_lasso_nrg, columns_enrg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(random_state=42)\n",
    "alpha_space = np.logspace(-5, 3, 15)   # Checking for alpha from .0001 to 1 and finding the best value for alpha\n",
    "parameters = {\"alpha\" : alpha_space}\n",
    "grid_ridge_nrg = GridSearchCV(ridge, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_ridge_nrg.fit(X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_nrg, y_pred_ridge = evaluate_prediction(label_1,X_nrg_test, y_nrg_test, metrics_df_nrg, grid_ridge_nrg, X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fetures_coef(grid_ridge_nrg, columns_enrg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elnet = ElasticNet(max_iter=20000,random_state=42)\n",
    "alpha_space = np.logspace(-5, 3, 15)   # Checking for alpha from .0001 to 1 and finding the best value for alpha\n",
    "l1_space = np.logspace(-3, 0, 15)\n",
    "parameters = {\"alpha\" : alpha_space,\n",
    "            \"l1_ratio\" : l1_space}\n",
    "grid_elnet_nrg = GridSearchCV(elnet, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_elnet_nrg.fit(X_nrg_train, y_nrg_train)\n",
    "\n",
    "metrics_df_nrg, y_pred_elnet = evaluate_prediction(label_1, X_nrg_test, y_nrg_test, metrics_df_nrg, grid_elnet_nrg, X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_errors(y_nrg_test, y_pred_elnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_discrete_feature(df_nrg_selection, 'PropertyGFATotal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features_next = ['BuildingType',\n",
    " 'PrimaryPropertyType',\n",
    " 'Neighborhood',\n",
    " 'LargestPropertyUseType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une figure avec des subplots\n",
    "fig, axs = plt.subplots(nrows=len(cat_features_next), ncols=1, figsize=(8, 4 * len(cat_features_next)), gridspec_kw={'hspace': 0.6})\n",
    "axs[0].set_ylabel('Moyenne des erreurs')\n",
    "\n",
    "# Boucle sur les features\n",
    "for i, feature in enumerate(cat_features_next):\n",
    "    # Récupérer l'axe courant\n",
    "    ax = axs[i]\n",
    "\n",
    "    # Calculer les erreurs moyennes pour chaque catégorie\n",
    "    df_nrg_selection[\"error_pred\"] = abs((np.array(y_nrg_test).reshape(-1,) - y_pred_elnet))\n",
    "    df_grouped = df_nrg_selection.groupby(feature).mean()['error_pred']\n",
    "    counts = df_nrg_selection[feature].value_counts()\n",
    "\n",
    "    # Afficher les barres de la moyenne des erreurs\n",
    "    ax.bar(df_grouped.index, df_grouped.values)\n",
    "\n",
    "    # Ajouter le nombre d'occurrences sur les barres de la moyenne des erreurs\n",
    "    for i, v in enumerate(df_grouped.values):\n",
    "        ax.text(i, v, str(counts[df_grouped.index[i]]), ha='center', va='bottom')\n",
    "\n",
    "    # Ajouter un titre pour chaque subplot\n",
    "    ax.set_title(feature)\n",
    "\n",
    "    # Ajouter un espace supplémentaire de 10% au-dessus et en dessous des barres du graphique\n",
    "    ax.margins(y=0.1)\n",
    "    \n",
    "    # Ajouter des étiquettes d'axe plus petites et penchées\n",
    "    ax.set_xticklabels(df_grouped.index, rotation=45, fontsize=7)\n",
    "\n",
    "# Ajouter un titre pour le graphique global\n",
    "fig.suptitle('Erreurs en fonction des variables catégorielles', y=0.95, fontsize=16)\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree = DecisionTreeRegressor(random_state=42)\n",
    "parameters = {'min_samples_leaf': [1, 2, 3],\n",
    "            'max_depth': range(5,10)}\n",
    "\n",
    "grid_dectree_nrg = GridSearchCV(dectree, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_dectree_nrg.fit(X_nrg_train, y_nrg_train)\n",
    "\n",
    "metrics_df_nrg, y_pred_dectree = evaluate_prediction(label_1,X_nrg_test, y_nrg_test, metrics_df_nrg, grid_dectree_nrg, X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_errors(y_nrg_test, y_pred_dectree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestRegressor(random_state=42)\n",
    "parameters = {'min_samples_leaf': [1, 2, 3],\n",
    "            'criterion': [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"],\n",
    "            'max_features': [0.3,0.5,0.75,1,\"sqrt\",\"log2\"]}\n",
    "\n",
    "grid_randomforest_nrg = GridSearchCV(randomforest, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_randomforest_nrg.fit(X_nrg_train, y_nrg_train.values.ravel())\n",
    "\n",
    "metrics_df_nrg, y_pred_randomforest = evaluate_prediction(label_1,X_nrg_test, y_nrg_test, metrics_df_nrg, grid_randomforest_nrg, X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_errors(y_nrg_test, y_pred_randomforest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le ColumnTransformer pour gérer les différentes colonnes avec des normalizers\n",
    "# Définir les normalizers à tester\n",
    "scalers = [\n",
    "    ('StandardScaler', StandardScaler()),\n",
    "    ('MinMaxScaler', MinMaxScaler()),\n",
    "    ('MaxAbsScaler', MaxAbsScaler())\n",
    "]\n",
    "\n",
    "# Définir les paramètres pour le GridSearchCV\n",
    "parameters = {\n",
    "    'scaler': [scaler for _, scaler in scalers]\n",
    "}\n",
    "\n",
    "# Créer le pipeline avec le scaler et le modèle\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('xgboost', xgboost.XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "grid_xgboost_nrg = GridSearchCV(pipeline, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_xgboost_nrg.fit(X_nrg_train, y_nrg_train)\n",
    "\n",
    "metrics_df_nrg, y_pred_xgboost = evaluate_prediction(label_1, X_nrg_test, y_nrg_test, metrics_df_nrg, grid_xgboost_nrg, X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGboost = xgboost.XGBRegressor(random_state=42)\n",
    "eta = np.logspace(-5, 0.3, 5)\n",
    "L1_reg = np.logspace(-10, 10, 5)\n",
    "L2_reg = np.logspace(-10, 10, 5)\n",
    "parameters = {'max_depth': [3,7,11],\n",
    "            'learning_rate': eta,\n",
    "            'reg_alpha': L1_reg,\n",
    "            'reg_lambda' : L2_reg,\n",
    "            'colsample_bytree' : [0.25,0.5,0.75,1]\n",
    "            }\n",
    "\n",
    "grid_xgboost_nrg = GridSearchCV(XGboost, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_xgboost_nrg.fit(X_nrg_train, y_nrg_train)\n",
    "\n",
    "metrics_df_nrg, y_pred_xgboost = evaluate_prediction(label_1, X_nrg_test, y_nrg_test, metrics_df_nrg, grid_xgboost_nrg,X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_errors(y_nrg_test, y_pred_xgboost)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightboost = LGBMRegressor(random_state=42)\n",
    "parameters = {'n_estimators': range(1000,1500,100),\n",
    "            'learning_rate': [0.01,0.001,0.03,0.1,0.3]}\n",
    "\n",
    "grid_lightboost_nrg = GridSearchCV(lightboost, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_lightboost_nrg.fit(X_nrg_train, y_nrg_train)\n",
    "\n",
    "metrics_df_nrg, y_pred_lgboost = evaluate_prediction(label_1,X_nrg_test, y_nrg_test, metrics_df_nrg, grid_lightboost_nrg, X_nrg_train, y_nrg_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label 2 - GHE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor()\n",
    "parameters = {\"strategy\" : (\"mean\",\"median\")}\n",
    "grid_dummy_ghe = GridSearchCV(dummy_regr, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_dummy_ghe.fit(X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_ghe, y_pred_dummy_ghe = evaluate_prediction(label_2,X_ghe_test, y_ghe_test, metrics_df_ghe, grid_dummy_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'fit_intercept': [True, False]}\n",
    "grid_reglin_ghe = GridSearchCV(reg,\n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_reglin_ghe.fit(X_ghe_train, y_ghe_train)\n",
    "\n",
    "metrics_df_ghe, y_pred_reg_ghe = evaluate_prediction(label_2,X_ghe_test, y_ghe_test, metrics_df_ghe, grid_reglin_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter=10000, tol=0.0001,random_state=42)\n",
    "alpha_space = np.logspace(-5, 5, 15)   # Checking for alpha from .0001 to 10000 and finding the best value for alpha\n",
    "parameters = {\"alpha\" : alpha_space}\n",
    "grid_lasso_ghe = GridSearchCV(lasso, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_lasso_ghe.fit(X_ghe_train, y_ghe_train)\n",
    "\n",
    "metrics_df_ghe, y_pred_lasso_ghe = evaluate_prediction(label_2, X_ghe_test, y_ghe_test, metrics_df_ghe, grid_lasso_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fetures_coef(grid_lasso_ghe, columns_ghe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vérification du scaling des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "for feat_idx in range(X_enrg_test.shape[1]):\n",
    "    ax = fig.add_subplot(7,5, (feat_idx+1))\n",
    "    h = ax.hist(X_nrg_test[:, feat_idx], bins=50, color='steelblue', density=True, edgecolor='none')\n",
    "    ax.set_title(columns_enrg[feat_idx], fontsize=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(random_state=42)\n",
    "alpha_space = np.logspace(-5, 3, 15)   # Checking for alpha from .0001 to 1 and finding the best value for alpha\n",
    "parameters = {\"alpha\" : alpha_space}\n",
    "grid_ridge_ghe = GridSearchCV(ridge, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_ridge_ghe.fit(X_ghe_train, y_ghe_train)\n",
    "\n",
    "metrics_df_ghe, y_pred_ridge_ghe = evaluate_prediction(label_2,X_ghe_test, y_ghe_test, metrics_df_ghe, grid_ridge_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fetures_coef(grid_ridge_nrg, columns_enrg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elnet = ElasticNet(max_iter=20000,random_state=42)\n",
    "alpha_space = np.logspace(-5, 3, 15)   # Checking for alpha from .0001 to 1 and finding the best value for alpha\n",
    "l1_space = np.logspace(-3, 0, 15)\n",
    "parameters = {\"alpha\" : alpha_space,\n",
    "            \"l1_ratio\" : l1_space}\n",
    "grid_elnet_ghe = GridSearchCV(elnet, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_elnet_ghe.fit(X_ghe_train, y_ghe_train)\n",
    "\n",
    "metrics_df_ghe, y_pred_elnet_ghe = evaluate_prediction(label_2,X_ghe_test, y_ghe_test, metrics_df_ghe, grid_elnet_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree = DecisionTreeRegressor(random_state=42)\n",
    "parameters = {'min_samples_leaf': [1, 2, 3,4,5],\n",
    "            'max_depth': range(5,10)}\n",
    "\n",
    "grid_dectree_ghe = GridSearchCV(dectree, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_dectree_ghe.fit(X_ghe_train, y_ghe_train)\n",
    "\n",
    "metrics_df_ghe, y_pred_dectree_ghe = evaluate_prediction(label_2,X_ghe_test, y_ghe_test, metrics_df_ghe, grid_dectree_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestRegressor(random_state=42)\n",
    "parameters = {'min_samples_leaf': [1, 2, 3],\n",
    "            'criterion': [\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"],\n",
    "            'max_features': [0.3,0.5,0.75,1,\"sqrt\",\"log2\"]}\n",
    "\n",
    "grid_randomforest_ghe = GridSearchCV(randomforest, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_randomforest_ghe.fit(X_ghe_train, y_ghe_train.values.ravel())\n",
    "\n",
    "metrics_df_ghe, y_pred_forest_ghe = evaluate_prediction(label_2,X_ghe_test, y_ghe_test, metrics_df_ghe, grid_randomforest_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'n_estimators': range(1000,1500,100),\n",
    "            'eta': [0.01,0.001,0.03,0.1,0.3],\n",
    "            'max_features': [0.3,0.5,0.75,1,\"sqrt\",\"log2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "\n",
    "grid_xgboost_ghe = GridSearchCV(XGBoost, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_xgboost_ghe.fit(X_ghe_train, y_ghe_train)\n",
    "\n",
    "metrics_df_ghe, y_pred_xgboost_ghe = evaluate_prediction(label_2, X_ghe_test, y_ghe_test, metrics_df_ghe, grid_xgboost_ghe, X_ghe_train, y_ghe_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightboost = LGBMRegressor(random_state=42)\n",
    "parameters = {'n_estimators': range(1000,1500,100),\n",
    "            'learning_rate': [0.01,0.001,0.03,0.1,0.3]}\n",
    "\n",
    "grid_lightboost_ghe = GridSearchCV(lightboost, \n",
    "                            param_grid=parameters,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1, \n",
    "                            scoring=scoring,\n",
    "                            refit=\"r2\")\n",
    "grid_lightboost_ghe.fit(X_ghe_train, y_ghe_train)\n",
    "\n",
    "metrics_df_ghe, y_pred_lgboost_ghe = evaluate_prediction(label_2,X_ghe_test, y_ghe_test, metrics_df_ghe, grid_lightboost_ghe, X_ghe_train, y_ghe_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
